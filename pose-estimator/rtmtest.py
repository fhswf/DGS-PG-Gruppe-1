"""
üîÑ Aktualisiertes Test-Skript f√ºr die komplette 2D‚Üí3D Pipeline

EINFACHE ERKL√ÑRUNG:
Dies ist unser "Hauptprogramm", das alle Teile zusammenf√ºgt wie ein Rezept:
1. üñºÔ∏è  Nimm ein Bild
2. ü§ñ  Finde Menschen darin (2D)
3. üîÑ  Mache es 3D-f√§hig mit TRAINIERTEM MODELL
4. üé®  Zeichne und zeige das Ergebnis

ZUSAMMENHANG:
Dieses Skript verbindet alle Komponenten unserer 3D-Pose-Pipeline:
- 2D Pose Estimation (OpenPose/MediaPipe)
- 3D Pose Lifting (trainiertes neuronales Netz)
- Visualisierung der Ergebnisse
"""

# ===============================================
# üì¶ IMPORTIEREN DER EIGENEN MODULE
# ===============================================
from pose_estimator_2d import PoseEstimator2D, DEFAULT_IGNORE_KEYPOINTS, filter_keypoints
from pose_estimator_3d import Pose3DConverter  # üîÑ Neue Converter-Klasse
# from pose_3d_visualizer_updated import plot_3d_pose_from_json, plot_multiple_views  # Optional
import json
import numpy as np

# ===============================================
# üöÄ WILLKOMMENSNACHRICHT
# ===============================================
print("=" * 80)
print("üéØ TEST: Komplette 2D ‚Üí 3D K√∂rperpositions-Pipeline mit AI-Modell")
print("=" * 80)
print("üìã Dieses Programm f√ºhrt alle Schritte automatisch durch:")
print("   1. üñºÔ∏è  Finde Menschen im Bild (2D)")
print("   2. ü§ñ Nutze trainiertes MLP-Modell f√ºr 3D-Konvertierung")
print("   3. üé® Speichere und zeige die Ergebnisse")
print("=" * 80)

# ===============================================
# üñºÔ∏è SCHRITT 0: BILD AUSW√ÑHLEN
# ===============================================
print("\nüì∏ SCHRITT 0: W√§hle ein Testbild aus")

# üñºÔ∏è TESTBILDER - Nur eine Zeile sollte aktiv sein (kein # davor):
file = "V.png"        # üü¶ Beispielbild 1 - Person mit V-Hand-Zeichen
# file = "hocke.jpg"    # üü¶ Beispielbild 2 - Person in der Hocke
# file = "mensch.jpg"   # üü¶ Beispielbild 3 - Standard-Person

print(f"‚úÖ Ausgew√§hltes Bild: {file}")

# ===============================================
# üéØ SCHRITT 1: 2D-K√ñRPERPOSITIONEN FINDEN (OPENPOSE/MEDIAPIPE)
# ===============================================
print("\n" + "="*80)
print("üì∏ SCHRITT 1: Finde Menschen im Bild (2D Pose Estimation)")
print("="*80)

print(f"üö´ Ignoriere Punkte: {DEFAULT_IGNORE_KEYPOINTS} (Beine/F√º√üe)")
"""
WARUM WIR BEINE IGNORIEREN:
- Bein-Posen sind oft ungenau in 2D-Detektoren
- Sie machen die 3D-Rekonstruktion instabil
- Fokus auf Oberk√∂rper f√ºr bessere Ergebnisse
- Punkte 13-22: Knie, Kn√∂chel, F√º√üe
"""

# ü§ñ Erstelle den 2D-"Body-Detektor" (OpenPose oder MediaPipe)
estimator_2d = PoseEstimator2D(kpt_threshold=0.9)
"""
KONFIGURATION:
- kpt_threshold=0.9: Nur Keypoints mit 90%+ Confidence werden akzeptiert
- H√∂herer Threshold = pr√§zisere aber weniger Keypoints
- Niedrigerer Threshold = mehr Keypoints aber m√∂glicherweise ungenauer
"""
print("‚úÖ 2D-Pose-Estimator erstellt (sehr hohe Genauigkeit)")

# üéØ Bild analysieren - Hier passiert die eigentliche 2D-Erkennung
result_2d = estimator_2d.process_image(file)
"""
WAS PASSIERT HIER:
1. Bild wird geladen und vorverarbeitet
2. Neuronales Netz findet Personen und ihre Gelenke
3. Keypoints werden in Koordinaten umgewandelt
4. Confidence-Scores werden berechnet
"""
print(f"üë§ Gefunden: {result_2d.num_persons} Person(en) im Bild")

# üîç Zeige Ersetzungen (falls welche durchgef√ºhrt wurden)
if result_2d.num_persons > 0:
    print("\nüîç KI-Ersetzungen (automatisch korrigiert):")
    print(f"   üëÉ Nase (Punkt 0):  Punkt 53 (Gesichts-Nase)")
    print(f"   ‚úã Linkes Handgelenk: Punkt 9 ‚Üí Punkt 91")
    print(f"   ‚úã Rechtes Handgelenk: Punkt 10 ‚Üí Punkt 112")
    """
    WARUM ERSETZUNGEN:
    - Standard-Body-Modelle haben oft Probleme mit bestimmten Keypoints
    - MediaPipe/Gesichts-Modelle sind f√ºr Gesichts-Posen besser
    - Hand-Modelle sind f√ºr Hand-Posen besser
    - Kombination verschiedener Modelle f√ºr bessere Ergebnisse
    """

# üö´ Filtere Beine heraus - Entferne unzuverl√§ssige Keypoints
print(f"\nüö´ Filtere Beine/F√º√üe (Punkte 13-22) aus...")
result_2d.keypoints, result_2d.scores = filter_keypoints(
    result_2d.keypoints,      # 2D-Koordinaten aller Personen
    result_2d.scores,         # Confidence-Scores
    DEFAULT_IGNORE_KEYPOINTS  # Welche Punkte zu filtern sind
)
"""
WAS filter_keypoints MACHT:
- Setzt die Koordinaten der ignorierten Keypoints auf 0
- Setzt ihre Confidence-Scores auf 0
- Erhaltene Keypoints bleiben unver√§ndert
"""

# ‚úÖ √úberpr√ºfung der Filterung
if result_2d.num_persons > 0:
    # Extrahiere Scores der gefilterten Keypoints
    filtered_scores = result_2d.scores[0][DEFAULT_IGNORE_KEYPOINTS]
    # Z√§hle wie viele auf 0 gesetzt wurden
    filtered_count = np.sum(filtered_scores == 0)
    print(f"‚úÖ {filtered_count} Bein-Punkte wurden 'unsichtbar' gemacht")

# ===============================================
# üíæ SCHRITT 2: 2D-DATEN SPEICHERN (JSON-FORMAT)
# ===============================================
print("\nüíæ Speichere 2D-Daten f√ºr 3D-Konvertierung...")

"""
JSON-STRUKTUR F√úR 3D-KONVERTIERUNG:
[
    {
        "frame": 0,                    # Frame-Nummer
        "left": {                      # Linke Kamera-Ansicht
            "num_persons": 1,         # Anzahl Personen
            "keypoints": [[x,y], ...], # 133√ó2 Koordinaten
            "scores": [0.9, ...],     # 133 Confidence-Werte
            "bboxes": [...]            # Bounding Boxes
        },
        "right": {...}                 # Rechte Kamera-Ansicht (gleiche Daten)
    }
]
"""
results_2d_list = [{
    "frame": 0,
    "left": {
        "num_persons": result_2d.num_persons,
        "keypoints": result_2d.keypoints.tolist(),  # np.array ‚Üí Liste
        "scores": result_2d.scores.tolist(),
        "bboxes": result_2d.bboxes.tolist()
    },
    "right": {  # Gleiche Daten f√ºr rechte Ansicht (Stereo-Kamera-Simulation)
        "num_persons": result_2d.num_persons,
        "keypoints": result_2d.keypoints.tolist(),
        "scores": result_2d.scores.tolist(),
        "bboxes": result_2d.bboxes.tolist()
    }
}]

# Speichere JSON-Datei
with open("poses_2d_filtered.json", "w") as f:
    json.dump(results_2d_list, f, indent=2)  # indent=2 f√ºr lesbares Format
print("‚úÖ Gespeichert: poses_2d_filtered.json")

# ===============================================
# üñçÔ∏è SCHRITT 3: 2D-BILD ANNOTIEREN (VISUALISIERUNG)
# ===============================================
print("\n" + "="*80)
print("üñçÔ∏è SCHRITT 3: Zeichne K√∂rperlinien auf das Originalbild")
print("="*80)

"""
WAS DIE ANNOTATION MACHT:
- Zeichnet Gelenke als Punkte
- Verbindet Gelenke mit Linien (Skelett)
- Zeichnet Bounding Boxes um Personen
- Ignoriert gefilterte Keypoints (Beine)
"""
bild = estimator_2d.process_image_with_annotation(
    image_path=file,                    # Eingabebild
    output_path="image_annotated_filtered.png",  # Ausgabedatei
    ignore_keypoints=DEFAULT_IGNORE_KEYPOINTS,  # Nicht zeichnen
    draw_bbox=True,                     # Bounding Box zeichnen
    draw_keypoints=True,                # Keypoints zeichnen
    keypoint_threshold=0.3              # Min. Confidence f√ºr Anzeige
)
print("üíæ Gespeichert: image_annotated_filtered.png")

# ===============================================
# üîÑ SCHRITT 4: 2D ‚Üí 3D MIT TRAINIERTEM MODELL (KERN-SCHRITT)
# ===============================================
print("\n" + "="*80)
print("ü™Ñ SCHRITT 4: 2D ‚Üí 3D mit trainiertem MLP-Modell")
print("="*80)
print("ü§ñ HIER PASSIERT DIE 'MAGIE':")
print("   - Das trainierte neuronale Netz wird geladen")
print("   - Es wandelt 2D-Koordinaten in 3D-Koordinaten um")
print("   - Tiefeninformationen (Z-Achse) werden berechnet")
print("   - Ergebnisse werden gespeichert")
print("="*80)

# ü§ñ Erstelle 3D-Converter mit trainiertem Modell
converter_3d = Pose3DConverter(
    model_path='lifting2DTo3D.pth',  # üìÅ Dein trainiertes Modell
    lifting_method='mlp',             # ü§ñ Nutze das AI-Modell!
    device='cuda',                    # üíª GPU falls verf√ºgbar, sonst 'cpu'
    ignore_keypoints=DEFAULT_IGNORE_KEYPOINTS  # Gleiche Filter wie bei 2D
)
"""
PARAMETER-ERKL√ÑRUNG:
- model_path: Pfad zur .pth Datei mit den trainierten Gewichten
- lifting_method: 'mlp' f√ºr neuronales Netz, 'geometric' f√ºr Fallback
- device: 'cuda' f√ºr NVIDIA GPU (schneller), 'cpu' f√ºr CPU (langsamer)
- ignore_keypoints: Welche Keypoints in der Ausgabe ignoriert werden sollen
"""

print(f"‚úÖ 3D-Converter initialisiert")
print(f"   Methode: {converter_3d.lifting_method}")
print(f"   Device: {converter_3d.device}")

# üîÑ Konvertiere die JSON-Datei von 2D zu 3D
print("\nüîÑ Starte Konvertierung...")
results_3d = converter_3d.convert_2d_json_to_3d(
    input_json_path="poses_2d_filtered.json",   # Eingabe: 2D-Posen
    output_json_path="poses_3d_mlp.json",       # Ausgabe: 3D-Posen
    image_size=(1920, 1080)  # Passe an deine Bildgr√∂√üe an!
)
"""
WAS convert_2d_json_to_3d MACHT:
1. L√§dt die 2D-JSON-Datei
2. F√ºr jeden Frame und jede Person:
   a. Extrahiert 2D-Koordinaten und Scores
   b. F√ºhrt Forward-Pass durch das neuronale Netz durch
   c. Berechnet 3D-Koordinaten (x, y, z)
   d. Speichert Ergebnisse mit Konfidenzen und Metadaten
3. Speichert alles in einer neuen JSON-Datei
"""

print("‚úÖ 3D-Modelle erfolgreich erstellt mit MLP!")

# üìä Zeige detaillierte Statistiken der Ergebnisse
if len(results_3d) > 0:
    frame0 = results_3d[0]['combined_3d']  # Extrahiere Daten von Frame 0
    print(f"\nüìä 3D-Ergebnis-Statistiken:")
    print(f"   üë• Personen: {frame0['num_persons']}")
    print(f"   üîß Methode: {frame0['method']}")
    print(f"   üéØ Genauigkeit: {frame0['confidence']:.1%}")
    
    # üîç Analysiere Tiefeninformationen (Z-Koordinaten)
    kpts_3d = np.array(frame0['keypoints_3d'])  # Konvertiere zu numpy array
    if len(kpts_3d) > 0 and frame0['num_persons'] > 0:
        z_coords = kpts_3d[0, :, 2]  # Extrahiere Z-Werte der ersten Person
        z_valid = z_coords[z_coords != 0]  # Ignoriere 0-Werte (gefilterte/ung√ºltige)
        
        if len(z_valid) > 0:
            print(f"   üìè Tiefenbereich: {z_valid.min():.2f} bis {z_valid.max():.2f}")
            print(f"   üìê Durchschnitt Z: {z_valid.mean():.2f}")
            """
            INTERPRETATION DER Z-WERTE:
            - Positive Z: Vorw√§rts (weg von der Kamera)
            - Negative Z: R√ºckw√§rts (zur Kamera hin)
            - Gr√∂√üere Werte: Weiter entfernt
            - Kleinere Werte: N√§her an der Kamera
            - 0: Keine Tiefeninformation verf√ºgbar
            """

# ===============================================
# üé® SCHRITT 5: 3D DIREKT VISUALISIEREN (Optional)
# ===============================================
print("\n" + "="*80)
print("üé® SCHRITT 5: Visualisierung der 3D-Posen")
print("="*80)

# Versuche den 3D-Visualizer zu importieren (optional)
try:
    from pose_3d_visualizer_updated import plot_3d_pose_from_json
    
    print("üñ•Ô∏è  Erstelle 3D-Visualisierung...")
    plot_3d_pose_from_json(
        "poses_3d_mlp.json",   # Eingabe: 3D-Posen JSON
        frame_idx=0,           # Welcher Frame visualisiert werden soll
        view='combined_3d',    # Welche Ansicht (left_3d, right_3d, combined_3d)
        output_path="image_3d_mlp.png",  # Wo gespeichert wird
        z_scale=5.0,           # Skalierung der Z-Achse f√ºr bessere Darstellung
        show_plot=True,        # Plot direkt anzeigen
        show_hands=True,       # H√§nde anzeigen
        show_face=True         # Gesicht anzeigen
    )
    print("üíæ Gespeichert: image_3d_mlp.png")
except ImportError:
    print("‚ö†Ô∏è  Visualizer nicht gefunden - √ºberspringe Visualisierung")
    print("   3D-Daten wurden aber gespeichert in: poses_3d_mlp.json")
    print("   Du kannst sie mit anderen Tools visualisieren (z.B. Blender, Unity)")

# ===============================================
# üéâ ZUSAMMENFASSUNG DER ERGEBNISSE
# ===============================================
print("\n" + "="*80)
print("üéâ FERTIG! Pipeline mit trainiertem MLP-Modell erfolgreich!")
print("="*80)

print("\nüìÅ ERSTELLTE DATEIEN:")
print("   1. üìÑ poses_2d_filtered.json        - Rohdaten der 2D-Erkennung")
print("   2. üñºÔ∏è  image_annotated_filtered.png  - Bild mit 2D-Skelett")
print("   3. üìÑ poses_3d_mlp.json             - 3D-Koordinaten (neuronales Netz)")
print("   4. üé® image_3d_mlp.png              - 3D-Visualisierung (falls verf√ºgbar)")

print("\nü§ñ VERWENDETES MODELL:")
print("   - Typ: Multi-Layer Perceptron (MLP)")
print("   - Architektur: 6 Schichten mit Residual Connections")
print("   - Datei: lifting2DTo3D.pth (trainierte Gewichte)")
print("   - Trainiert auf: H3WB Dataset (133 Keypoints)")
print("   - Eingabe: 266 Werte (133√ó2), Ausgabe: 399 Werte (133√ó3)")

print("\nüí° N√ÑCHSTE SCHRITTE / PROBLEML√ñSUNG:")
print("   1. Falls Modell fehlt: Trainiere es mit train_new_model()")
print("   2. Falls ungenau: Trainiere mit mehr Daten oder mehr Epochen")
print("   3. Teste verschiedene Bilder f√ºr verschiedene Posen")
print("   4. Vergleiche 'geometric' vs 'mlp' Methode f√ºr Qualit√§tscheck")

# ===============================================
# üìä ALTERNATIVE: DIREKTER VERGLEICH BEIDER METHODEN
# ===============================================
print("\n" + "="*80)
print("üî¨ BONUS: Vergleich Geometric vs MLP Methode")
print("="*80)
print("ü§î WARUM VERGLEICHEN?")
print("   - Geometric: Einfache Heuristik (Fallback)")
print("   - MLP: Neuronales Netz (trainiert auf echten Daten)")
print("   - Vergleich zeigt Qualit√§tsunterschiede")
print("="*80)

# Geometric Methode zum Vergleich (Fallback ohne Training)
converter_geometric = Pose3DConverter(
    lifting_method='geometric',  # Einfache Heuristik statt MLP
    ignore_keypoints=DEFAULT_IGNORE_KEYPOINTS
)

# Konvertiere mit geometrischer Methode
results_3d_geometric = converter_geometric.convert_2d_json_to_3d(
    input_json_path="poses_2d_filtered.json",
    output_json_path="poses_3d_geometric.json",
    image_size=(1920, 1080)
)

print("\nüìä VERGLEICH DER KONFIDENZEN:")
# Extrahiere Konfidenzwerte beider Methoden
mlp_conf = results_3d[0]['combined_3d']['confidence']
geo_conf = results_3d_geometric[0]['combined_3d']['confidence']

print(f"   ü§ñ MLP Confidence:       {mlp_conf:.1%}")
print(f"   üìê Geometric Confidence: {geo_conf:.1%}")
print(f"   {'‚úÖ MLP ist besser!' if mlp_conf > geo_conf else '‚ö†Ô∏è  Geometric ist besser - Modell nachtrainieren?'}")

"""
INTERPRETATION:
- Confidence < 50%: Schlechte Erkennung, Modell ben√∂tigt mehr Training
- Confidence 50-70%: Akzeptable Ergebnisse
- Confidence 70-90%: Gute Ergebnisse
- Confidence > 90%: Exzellente Ergebnisse
"""

print("\n‚úÖ Alle Tests abgeschlossen!")
print("\nüîß TECHNISCHE HINWEISE:")
print("   - 2D-Erkennung: OpenPose/MediaPipe (CPU/GPU)")
print("   - 3D-Lifting: Eigenes neuronales Netz (PyTorch)")
print("   - Datenformat: JSON f√ºr einfache Weiterverarbeitung")
print("   - Visualisierung: Matplotlib/Open3D (optional)")