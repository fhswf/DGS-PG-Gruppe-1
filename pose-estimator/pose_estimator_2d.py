"""
ğŸ–¼ï¸ PoseEstimator2D: Eine Python-Klasse fÃ¼r 2D-KÃ¶rperpositions-Erkennung

EINFACHE ERKLÃ„RUNG:
Dieses Programm analysiert Bilder und Videos und findet darin Menschen.
Es zeigt an, wo sich KÃ¶rperteile wie Kopf, Arme, HÃ¤nde befinden.
Das ist wie eine digitale Version von "Mensch-Ã¤rgere-dich-nicht"-Figuren erkennen!

Funktioniert mit RTMLib (einer KI-Bibliothek fÃ¼r PosenschÃ¤tzung)
Liefert 133 KÃ¶rperpunkte pro Person:
- 17 Punkte fÃ¼r den KÃ¶rper
- 68 Punkte fÃ¼r das Gesicht  
- 42 Punkte fÃ¼r die HÃ¤nde
- 6 Punkte fÃ¼r die FÃ¼ÃŸe

Autor: DGS Project Group 1
Datum: September 2025
"""

# ===============================================
# ğŸ“¦ IMPORTIEREN DER BENÃ–TIGTEN BIBLIOTHEKEN
# ===============================================
import cv2  # ğŸ–¼ï¸ FÃ¼r Bilder und Videos (OpenCV - Computer Vision)
import numpy as np  # ğŸ”¢ FÃ¼r Zahlen und Berechnungen
from pathlib import Path  # ğŸ“ FÃ¼r Dateipfade und Ordner
from typing import Union, List, Tuple, Optional  # ğŸ“ FÃ¼r bessere Code-Lesbarkeit
from dataclasses import dataclass  # ğŸ—ï¸ FÃ¼r strukturierte Daten-Container
import json  # ğŸ“„ Zum Speichern im JSON-Format (lesbar fÃ¼r Mensch und Computer)
import time  # â±ï¸ FÃ¼r Zeitmessungen

try:
    # ğŸ¤– Versuche RTMLib zu laden (die KI-Motor)
    from rtmlib import Wholebody, draw_skeleton
except ImportError:
    # âŒ Falls nicht installiert: Installationsanleitung zeigen
    raise ImportError("RTMLib nicht gefunden. Installiere mit: pip install rtmlib")

# ===============================================
# âš™ï¸ KONFIGURATION: WELCHE KÃ–RPERTEILE WEGLASSEN?
# ===============================================
# StandardmÃ¤ÃŸig ignorierte KÃ¶rperpunkte: Beine, FÃ¼ÃŸe, Zehen (Punkte 13-22)
# Warum? Manchmal wollen wir uns nur auf OberkÃ¶rper konzentrieren!
DEFAULT_IGNORE_KEYPOINTS = list(range(13, 23))  # ğŸ”¢ Von Punkt 13 bis 22

# ===============================================
# ğŸ”§ HILFSFUNKTION 1: BESTIMMTE KÃ–RPERPUNKTE AUSSCHALTEN
# ===============================================
def filter_keypoints(keypoints, scores, ignore_indices=None):
    """
    ğŸ¯ SETZT BESTIMMTE KÃ–RPERPUNKTE AUF "UNSICHTBAR"
    
    EINFACH GESAGT:
    Diese Funktion macht bestimmte KÃ¶rperteile (z.B. Beine) unsichtbar,
    indem sie ihre Position auf (0,0) setzt und die Genauigkeit auf 0.
    
    BEISPIEL:
    Wenn wir nur OberkÃ¶rper analysieren wollen, schalten wir Beine aus.
    
    ğŸ”§ Parameter (Eingaben):
        keypoints: Liste von KÃ¶rperpunkt-Positionen
        scores: Liste von Genauigkeitswerten (wie sicher ist die KI?)
        ignore_indices: Welche Punkte sollen ignoriert werden?
    
    ğŸ“¤ RÃ¼ckgabe:
        Gefilterte keypoints und scores (Kopien der Originaldaten)
    """
    if ignore_indices is None:
        # ğŸš« Keine Filterung: Einfach Kopien zurÃ¼ckgeben
        return keypoints.copy(), scores.copy()
    
    # ğŸ“‹ Kopien der Originaldaten erstellen (wir Ã¤ndern Original NICHT!)
    keypoints_filtered = keypoints.copy()
    scores_filtered = scores.copy()
    
    # ğŸ”„ FÃ¼r jeden zu ignorierenden Punkt...
    for idx in ignore_indices:
        if idx < keypoints_filtered.shape[1]:  # âœ… PrÃ¼fen ob Punkt existiert
            keypoints_filtered[:, idx, :] = 0  # ğŸ¯ Position auf (0,0) setzen
            scores_filtered[:, idx] = 0        # ğŸ¯ Genauigkeit auf 0 setzen
    
    return keypoints_filtered, scores_filtered

# ===============================================
# ğŸ¨ HILFSFUNKTION 2: SKELETT-LINIEN ZEICHNEN
# ===============================================
def draw_skeleton_filtered(image, keypoints, scores, ignore_indices=None, kpt_thr=0.3):
    """
    ğŸ–ï¸ ZEICHNET KÃ–RPER-LINIEN OHNE IGNORIERTE BEREICHE
    
    EINFACH GESAGT:
    Malt grÃ¼ne Linien zwischen KÃ¶rperpunkten und rote Punkte auf die Positionen.
    Ãœberspringt dabei KÃ¶rperteile, die wir nicht sehen wollen (z.B. Beine).
    
    ğŸ–¼ï¸ Beispiel-Output:
        â—‹ Kopf
        â”œâ”€â”€â—‹ Linke Schulter
        â”‚  â””â”€â”€â—‹ Linker Ellbogen
        â”‚     â””â”€â”€â—‹ Linkes Handgelenk
        â””â”€â”€â—‹ Rechte Schulter
           â””â”€â”€â—‹ Rechter Ellbogen
              â””â”€â”€â—‹ Rechtes Handgelenk
    
    ğŸ”§ Parameter:
        image: Das Original-Bild (wird nicht verÃ¤ndert!)
        keypoints: KÃ¶rperpunkt-Positionen
        scores: Genauigkeitswerte
        ignore_indices: Zu ignorierende Punkte
        kpt_thr: Mindest-Genauigkeit zum Zeichnen (0.3 = 30% sicher)
    
    ğŸ“¤ RÃ¼ckgabe:
        Annotiertes Bild mit gezeichnetem Skelett
    """
    if ignore_indices is None:
        # ğŸ¨ Fallback: Verwende Standard-Zeichenfunktion von RTMLib
        from rtmlib import draw_skeleton
        return draw_skeleton(image, keypoints, scores, kpt_thr=kpt_thr)
    
    # ğŸ¦´ DEFINITION DER KÃ–RPER-VERBINDUNGEN (OHNE BEINE!)
    # Welche Punkte sollen mit Linien verbunden werden?
    BODY_CONNECTIONS = [
        (53, 1), (53, 2), (1, 3), (2, 4),  # ğŸ‘¤ Kopf (Punkt 53 = Nase)
        (3, 5), (4, 6), (5, 6),           # ğŸ¯ Schultern
        (5, 7), (7, 91),                  # ğŸ’ª Linker Arm
        (6, 8), (8, 112),                 # ğŸ’ª Rechter Arm
        (5, 11), (6, 12), (11, 12),       # ğŸ‹ï¸ Torso (OberkÃ¶rper)
    ]
    
    # ğŸ“‹ Kopie des Originalbildes (wir malen auf die Kopie!)
    annotated = image.copy()
    # âš¡ Schneller Zugriff: Set aus ignore_indices machen
    ignore_set = set(ignore_indices)
    
    # ğŸ‘¥ FÃ¼r jede Person im Bild...
    for person_idx in range(len(keypoints)):
        kpts = keypoints[person_idx]  # ğŸ“ Punkte dieser Person
        conf = scores[person_idx]     # ğŸ¯ Genauigkeiten dieser Person
        
        # ğŸ–ï¸ LINIEN ZEICHNEN (Verbindungen zwischen Punkten)
        for start_idx, end_idx in BODY_CONNECTIONS:
            # âœ… PrÃ¼fen: Beide Punkte NICHT ignoriert?
            if start_idx not in ignore_set and end_idx not in ignore_set:
                # âœ… PrÃ¼fen: Beide Punkte genug sicher?
                if conf[start_idx] > kpt_thr and conf[end_idx] > kpt_thr:
                    pt1 = tuple(kpts[start_idx].astype(int))  # ğŸ¯ Start-Punkt
                    pt2 = tuple(kpts[end_idx].astype(int))    # ğŸ¯ End-Punkt
                    # ğŸŸ¢ GrÃ¼ne Linie zeichnen (Farbe: 0,255,0, Dicke: 1)
                    cv2.line(annotated, pt1, pt2, (0, 255, 0), 1)
        
        # ğŸ”´ PUNKTE ZEICHNEN (Einzelne KÃ¶rperpunkte)
        for idx in range(len(kpts)):
            # âœ… PrÃ¼fen: Punkt nicht ignoriert und genug sicher?
            if idx not in ignore_set and conf[idx] > kpt_thr:
                pt = tuple(kpts[idx].astype(int))  # ğŸ¯ Punkt-Position
                # ğŸ”´ Roten Punkt zeichnen (Radius: 1, komplett ausgefÃ¼llt)
                cv2.circle(annotated, pt, 1, (0, 0, 255), -1)
    
    return annotated

# ===============================================
# ğŸ“¦ DATENKLASSE 1: ERGEBNIS FÃœR EIN EINZELBILD
# ===============================================
@dataclass
class PoseResult:
    """
    ğŸ·ï¸ EIN "DATEN-BEHÃ„LTER" FÃœR EINZELBILD-ERGEBNISSE
    
    Stell dir das vor wie ein digitales Formular, das alle Infos zu 
    einer Posenerkennung in einem Bild speichert.
    
    ğŸ“‹ INHALT:
        frame_idx:     Bild-Nummer (bei Videos)
        keypoints:     KÃ¶rperpunkt-Positionen [Personen, 133 Punkte, X/Y]
        scores:        Genauigkeiten fÃ¼r jeden Punkt [Personen, 133 Punkte]
        bboxes:        Begrenzungsrahmen um Personen [Personen, 5 Werte]
        num_persons:   Anzahl der gefundenen Personen
    """
    frame_idx: int
    keypoints: np.ndarray
    scores: np.ndarray
    bboxes: np.ndarray
    num_persons: int

# ===============================================
# ğŸ“¦ DATENKLASSE 2: ERGEBNIS FÃœR EIN GANZES VIDEO
# ===============================================
@dataclass
class VideoResult:
    """
    ğŸï¸ EIN "DATEN-BEHÃ„LTER" FÃœR VIDEO-ERGEBNISSE
    
    Speichert alle Einzelbild-Ergebnisse eines Videos plus Video-Infos.
    
    ğŸ“‹ INHALT:
        frame_results:    Liste von PoseResult fÃ¼r jedes Bild
        total_frames:     Anzahl aller verarbeiteten Bilder
        fps:              Bilder pro Sekunde im Original-Video
        processing_time:  Verarbeitungszeit in Sekunden
    """
    frame_results: List[PoseResult]
    total_frames: int
    fps: float
    processing_time: float

# ===============================================
# ğŸš€ HAUPTKLASSE: DER POSE-ESTIMATOR
# ===============================================
class PoseEstimator2D:
    """
    ğŸ¤– DIE HAUPTKLASSE FÃœR 2D-POSENERKENNUNG
    
    EINFACH GESAGT:
    Dies ist unser "digitaler Body-Detektor". Er kann:
    1. ğŸ–¼ï¸ In Bildern Menschen finden
    2. ğŸï¸ In Videos Menschen verfolgen
    3. ğŸ“ Genau zeigen, wo KÃ¶rperteile sind
    4. ğŸ’¾ Ergebnisse speichern und exportieren
    
    So nutzt du es:
        estimator = PoseEstimator2D(device='cpu')
        result = estimator.process_image("mein_bild.jpg")
    """
    
    def __init__(
        self,
        mode: str = 'performance',      # ğŸ¥‡ Beste Genauigkeit
        backend: str = 'onnxruntime',   # ğŸ—ï¸ KI-AusfÃ¼hrungs-Engine
        device: str = 'cpu',            # ğŸ’» Hardware (cpu, cuda fÃ¼r NVIDIA, mps fÃ¼r Apple)
        to_openpose: bool = False,      # ğŸ”€ OpenPose-Format konvertieren?
        kpt_threshold: float = 0.8      # ğŸ¯ Mindest-Genauigkeit fÃ¼r Punkte (80%)
    ):
        """
        ğŸ—ï¸ KONSTRUKTOR: INITIALISIERT DEN ESTIMATOR
        
        Hier wird der KI-Motor (RTMLib) gestartet und konfiguriert.
        """
        self.mode = mode
        self.backend = backend
        self.devend = device
        self.to_openpose = to_openpose
        self.kpt_threshold = kpt_threshold
        
        try:
            # ğŸ¤– RTMLib KI-Modell laden (133-Punkte-GanzkÃ¶rper-Modell)
            self.model = Wholebody(
                mode=mode,
                backend=backend,
                device=device,
                to_openpose=to_openpose
            )
            print(f"âœ… RTMLib Wholebody geladen mit:")
            print(f"   Modus: {mode}, Backend: {backend}, GerÃ¤t: {device}")
        except Exception as e:
            raise RuntimeError(f"âŒ RTMLib konnte nicht geladen werden: {e}")
    
    def _replace_keypoints(self, keypoints: np.ndarray, scores: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        ğŸ”„ ERSETZT BESTIMMTE KÃ–RPERPUNKTE DURCH GENAUERE
        
        WARUM?
        Die KI hat zwei Arten von Nasen- und Handgelenk-Punkten:
        1. Von KÃ¶rper-Erkennung (weniger genau)
        2. Von Gesichts-/Hand-Erkennung (genauer)
        
        ğŸ‘ƒ Beispiel Nase:
            - Punkt 0: KÃ¶rper-Nase (ungefÃ¤hr)
            - Punkt 53: Gesichts-Nase (genau)
            â†’ Wir nehmen Punkt 53!
        
        âœ‹ Beispiel Handgelenke:
            - Punkt 9: Linkes KÃ¶rper-Handgelenk
            - Punkt 91: Linkes Hand-Handgelenk (aus Hand-Erkennung)
            â†’ Wir nehmen Punkt 91!
        
        ğŸ”§ Parameter:
            keypoints: Alle 133 Punkte pro Person
            scores: Genauigkeiten aller Punkte
            
        ğŸ“¤ RÃ¼ckgabe:
            Verbesserte keypoints und scores
        """
        keypoints_modified = keypoints.copy()  # ğŸ“‹ Kopie
        scores_modified = scores.copy()        # ğŸ“‹ Kopie
        
        # ğŸ‘¥ FÃ¼r jede erkannte Person...
        for person_idx in range(len(keypoints)):
            # 1. ğŸ‘ƒ NASE ERSETZEN (Punkt 0 durch 53)
            if scores[person_idx, 53] > 0:  # âœ… Wenn Gesichts-Nase erkannt
                keypoints_modified[person_idx, 0] = keypoints[person_idx, 53]
                scores_modified[person_idx, 0] = scores[person_idx, 53]
            else:
                # ğŸš« Keine Gesichts-Nase: KÃ¶rper-Nase unsichtbar machen
                keypoints_modified[person_idx, 0] = 0
                scores_modified[person_idx, 0] = 0
            
            # 2. âœ‹ LINKES HANDGELENK ERSETZEN (9 durch 91)
            if scores[person_idx, 91] > 0:
                keypoints_modified[person_idx, 9] = keypoints[person_idx, 91]
                scores_modified[person_idx, 9] = scores[person_idx, 91]
            else:
                keypoints_modified[person_idx, 9] = 0
                scores_modified[person_idx, 9] = 0
            
            # 3. âœ‹ RECHTES HANDGELENK ERSETZEN (10 durch 112)
            if scores[person_idx, 112] > 0:
                keypoints_modified[person_idx, 10] = keypoints[person_idx, 112]
                scores_modified[person_idx, 10] = scores[person_idx, 112]
            else:
                keypoints_modified[person_idx, 10] = 0
                scores_modified[person_idx, 10] = 0
        
        return keypoints_modified, scores_modified
    
    def _process_frame(self, frame: np.ndarray, frame_idx: int = 0) -> PoseResult:
        """
        ğŸ¯ KERN-FUNKTION: ANALYSIERT EIN EINZELBILD
        
        Hier passiert die Magie: KI analysiert Bild â†’ findet Menschen â†’ berechnet Punkte.
        
        ğŸ”§ Parameter:
            frame: Das Bild als numpy Array (BGR Format)
            frame_idx: Bild-Nummer (fÃ¼r Videos wichtig)
            
        ğŸ“¤ RÃ¼ckgabe:
            PoseResult mit allen Ergebnissen
        """
        try:
            # ===============================================
            # ğŸ“¥ SCHRITT 1: BILD MIT KI ANALYSIEREN
            # ===============================================
            keypoints, scores = self.model(frame)  # ğŸ¤– KI sagt: "Hier sind Menschen!"
            
            # ğŸš« PrÃ¼fen: Wurden Ã¼berhaupt Personen gefunden?
            if keypoints is None or len(keypoints) == 0:
                return PoseResult(
                    frame_idx=frame_idx,
                    keypoints=np.empty((0, 133, 2)),  # ğŸ“­ Leeres Array: 0 Personen
                    scores=np.empty((0, 133)),
                    bboxes=np.empty((0, 5)),
                    num_persons=0
                )
            
            # ===============================================
            # ğŸ”¢ SCHRITT 2: DATEN IN RICHTIGES FORMAT BRINGEN
            # ===============================================
            keypoints = np.array(keypoints)  # ğŸ”„ In numpy Array umwandeln
            scores = np.array(scores)        # ğŸ”„ Genauigkeiten umwandeln
            
            # ğŸ“Š KI-Interne Werte in Prozente (0-100%) umrechnen
            logits = np.array(scores)
            confidence_scores = 1 / (1 + np.exp(-logits))  # ğŸ§® Mathe-Formel
            
            # ğŸ”§ Sicherstellen: Arrays haben richtige Dimensionen
            if keypoints.ndim == 2:
                keypoints = keypoints[np.newaxis, ...]  # ğŸ‘¥ Person-Dimension hinzufÃ¼gen
            
            if confidence_scores.ndim == 1:
                confidence_scores = confidence_scores[np.newaxis, ...]
            
            num_persons = keypoints.shape[0]  # ğŸ‘¥ Wie viele Personen?
            
            # ===============================================
            # ğŸ”„ SCHRITT 3: PUNKTE VERBESSERN (Genauere Versionen nehmen)
            # ===============================================
            keypoints, confidence_scores = self._replace_keypoints(keypoints, confidence_scores)
            
            # ===============================================
            # ğŸ“¦ SCHRITT 4: BEGRENZUNGSRAHMEN BERECHNEN
            # ===============================================
            # ğŸ”² GrÃ¼ne Rechtecke um jede Person berechnen
            bboxes = []
            for i in range(num_persons):
                kpts = keypoints[i].copy()  # ğŸ“ Punkte dieser Person
                conf_scores_flat = confidence_scores[i]  # ğŸ¯ Genauigkeiten
                
                # ğŸš« Punkte mit niedriger Genauigkeit ignorieren
                low_confidence_mask = conf_scores_flat <= self.kpt_threshold
                kpts[low_confidence_mask, 0] = 0  # X-Koordinate auf 0
                kpts[low_confidence_mask, 1] = 0  # Y-Koordinate auf 0
                keypoints[i] = kpts  # ğŸ“‹ ZurÃ¼ckspeichern
                
                # âœ… Finde gÃ¼ltige Punkte (nicht 0,0)
                non_zero_mask = (kpts != 0).any(axis=1)
                valid_kpts = kpts[non_zero_mask]
                
                if len(valid_kpts) > 0:
                    # ğŸ“ Rechteck berechnen: min/max von X und Y
                    x_coords = valid_kpts[:, 0]
                    y_coords = valid_kpts[:, 1]
                    x1, y1 = np.min(x_coords), np.min(y_coords)  # â†–ï¸ Oben links
                    x2, y2 = np.max(x_coords), np.max(y_coords)  # â†˜ï¸ Unten rechts
                    
                    # â¬œ 20 Pixel Rand hinzufÃ¼gen
                    padding = 20
                    x1 = max(0, x1 - padding)          # Nicht kleiner als 0
                    y1 = max(0, y1 - padding)          # Nicht kleiner als 0
                    x2 = min(frame.shape[1], x2 + padding)  # Nicht breiter als Bild
                    y2 = min(frame.shape[0], y2 + padding)  # Nicht hÃ¶her als Bild
                    
                    # ğŸ¯ Durchschnitts-Genauigkeit berechnen
                    high_confidence_scores = conf_scores_flat[conf_scores_flat > self.kpt_threshold]
                    confidence = np.mean(high_confidence_scores) if len(high_confidence_scores) > 0 else 0
                    
                    # ğŸ“¦ Rahmen zur Liste hinzufÃ¼gen [x1, y1, x2, y2, confidence]
                    bboxes.append([x1, y1, x2, y2, confidence])
                else:
                    # ğŸš« Keine gÃ¼ltigen Punkte: Leeren Rahmen
                    bboxes.append([0, 0, 0, 0, 0])
            
            bboxes_array = np.array(bboxes)  # ğŸ”„ In numpy Array
            
            # ===============================================
            # ğŸ“Š SCHRITT 5: DEBUG-AUSGABE (FÃ¼r Entwickler)
            # ===============================================
            print(f"Frame {frame_idx}: Punkt 0 (Nase) = {keypoints[0, 0]}")
            print(f"Frame {frame_idx}: Punkt 53 (Gesichts-Nase) = {keypoints[0, 53]}")
            print(f"Frame {frame_idx}: Punkt 91 (Hand-Handgelenk) = {keypoints[0, 91]}")
            
            # ===============================================
            # ğŸ“¤ SCHRITT 6: ERGEBNIS ZURÃœCKGEBEN
            # ===============================================
            return PoseResult(
                frame_idx=frame_idx,
                keypoints=keypoints,          # ğŸ“ Verbesserte Punkte
                scores=confidence_scores,     # ğŸ¯ Genauigkeiten
                bboxes=bboxes_array,          # ğŸ”² Begrenzungsrahmen
                num_persons=num_persons       # ğŸ‘¥ Anzahl Personen
            )
            
        except Exception as e:
            # âŒ Falls Fehler: Fehlermeldung und leeres Ergebnis
            print(f"âŒ Fehler bei Frame {frame_idx}: {e}")
            return PoseResult(
                frame_idx=frame_idx,
                keypoints=np.empty((0, 133, 2)),
                scores=np.empty((0, 133)),
                bboxes=np.empty((0, 5)),
                num_persons=0
            )
    
    def process_image(self, image_path: Union[str, Path]) -> PoseResult:
        """
        ğŸ–¼ï¸ ANALYSIERT EIN EINZELNES BILD
        
        ğŸ”§ Parameter:
            image_path: Pfad zum Bild (jpg, png, etc.)
            
        ğŸ“¤ RÃ¼ckgabe:
            PoseResult mit den gefundenen Personen
            
        ğŸš¨ MÃ¶gliche Fehler:
            FileNotFoundError: Bild existiert nicht
            ValueError: Bild kann nicht geladen werden
        """
        image_path = Path(image_path)
        if not image_path.exists():
            raise FileNotFoundError(f"âŒ Bild nicht gefunden: {image_path}")
        
        # ğŸ–¼ï¸ Bild laden
        frame = cv2.imread(str(image_path))
        if frame is None:
            raise ValueError(f"âŒ Bild kann nicht geladen werden: {image_path}")
        
        print(f"ğŸ“¸ Verarbeite Bild: {image_path}")
        print(f"ğŸ“ GrÃ¶ÃŸe: {frame.shape[1]}x{frame.shape[0]} Pixel")
        
        # ğŸ¯ Bild analysieren
        result = self._process_frame(frame, frame_idx=0)
        
        print(f"ğŸ‘¤ Gefunden: {result.num_persons} Person(en)")
        return result
    
    def process_image_with_annotation(
        self,
        image_path: Union[str, Path],
        output_path: Optional[Union[str, Path]] = None,
        draw_bbox: bool = True,           # ğŸ”² GrÃ¼ne Rechtecke zeichnen?
        draw_keypoints: bool = True,      # ğŸ¯ Punkte und Linien zeichnen?
        keypoint_threshold: float = 0.3,  # ğŸ¯ Mindest-Genauigkeit fÃ¼r Zeichnen
        ignore_keypoints: Optional[List[int]] = None  # ğŸš« Zu ignorierende Punkte
    ) -> PoseResult:
        """
        ğŸ–¼ï¸ğŸ“ ANALYSIERT BILD UND SPEICHERT ANNOTIERTE VERSION
        
        ğŸ”§ Parameter:
            image_path: Pfad zum Eingabebild
            output_path: Wo annotiertes Bild speichern? (optional)
            draw_bbox: Begrenzungsrahmen zeichnen?
            draw_keypoints: Skelett zeichnen?
            keypoint_threshold: Wie sicher muss Punkt sein zum Zeichnen?
            ignore_keypoints: Welche Punkte ignorieren? (z.B. Beine)
            
        ğŸ“¤ RÃ¼ckgabe:
            PoseResult + gespeichertes Bild (falls output_path)
        """
        image_path = Path(image_path)
        
        if output_path is not None:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)  # ğŸ“ Ordner erstellen
        
        # ğŸ–¼ï¸ Bild laden
        frame = cv2.imread(str(image_path))
        if frame is None:
            raise ValueError(f"âŒ Bild kann nicht geladen werden: {image_path}")
        
        print(f"ğŸ“¸ Verarbeite Bild: {image_path}")
        
        # ğŸ¯ Bild analysieren
        result = self._process_frame(frame, frame_idx=0)
        
        # ğŸš« Optional: Bestimmte Punkte filtern (z.B. Beine)
        if ignore_keypoints is not None:
            result.keypoints, result.scores = filter_keypoints(
                result.keypoints, 
                result.scores, 
                ignore_keypoints
            )
        
        # ğŸ–ï¸ Kopie fÃ¼r Annotationen erstellen
        annotated_frame = frame.copy()
        
        # ğŸ‘¥ Falls Personen gefunden...
        if result.num_persons > 0:
            # ğŸ”² GrÃ¼ne Rechtecke zeichnen
            if draw_bbox and len(result.bboxes) > 0:
                for bbox in result.bboxes:
                    x1, y1, x2, y2 = bbox[:4].astype(int)  # ğŸ“ Koordinaten
                    # ğŸŸ© GrÃ¼nes Rechteck (Farbe: 0,255,0, Dicke: 2)
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            
            # ğŸ¯ Punkte und Skelett zeichnen
            if draw_keypoints:
                annotated_frame = draw_skeleton_filtered(
                    annotated_frame,
                    result.keypoints,
                    result.scores,
                    ignore_keypoints,
                    kpt_thr=keypoint_threshold
                )
        
        # ğŸ’¾ Annotiertes Bild speichern (falls gewÃ¼nscht)
        if output_path is not None:
            cv2.imwrite(str(output_path), annotated_frame)
            print(f"ğŸ’¾ Annotiertes Bild gespeichert: {output_path}")
        
        return result
    
    def process_video(
        self,
        video_path: Union[str, Path],
        output_dir: Optional[Union[str, Path]] = None,
        save_frames: bool = False,       # ğŸ–¼ï¸ Einzelbilder speichern?
        max_frames: Optional[int] = None # ğŸ”¢ Maximale Anzahl Bilder
    ) -> VideoResult:
        """
        ğŸï¸ ANALYSIERT EIN GANZES VIDEO
        
        ğŸ”§ Parameter:
            video_path: Pfad zum Video (mp4, avi, etc.)
            output_dir: Wo Ergebnisse speichern? (optional)
            save_frames: Einzelbilder mit Annotationen speichern?
            max_frames: Nur erste X Bilder analysieren (schneller Test)
            
        ğŸ“¤ RÃ¼ckgabe:
            VideoResult mit allen Einzelbild-Ergebnissen
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"âŒ Video nicht gefunden: {video_path}")
        
        # ğŸ¬ Video Ã¶ffnen
        cap = cv2.VideoCapture(str(video_path))
        if not cap.isOpened():
            raise ValueError(f"âŒ Video kann nicht geÃ¶ffnet werden: {video_path}")
        
        # ğŸ“Š Video-Eigenschaften lesen
        fps = cap.get(cv2.CAP_PROP_FPS)  # ğŸï¸ Bilder pro Sekunde
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # ğŸ”¢ Gesamtanzahl
        
        if max_frames:
            total_frames = min(total_frames, max_frames)  # ğŸ”¢ Begrenzen
        
        print(f"ğŸ¬ Verarbeite Video: {video_path}")
        print(f"ğŸ“Š FPS: {fps}, Gesamte Bilder: {total_frames}")
        
        # ğŸ“ Ausgabeordner vorbereiten
        if output_dir and save_frames:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
        
        # ğŸ“‹ Liste fÃ¼r alle Ergebnisse
        frame_results = []
        start_time = time.time()  # â±ï¸ Startzeit messen
        
        # ğŸ”„ Alle Bilder/Frames durchgehen
        for frame_idx in range(total_frames):
            ret, frame = cap.read()  # ğŸ“· NÃ¤chstes Bild lesen
            if not ret:  # ğŸ Videoende erreicht?
                break
            
            # ğŸ¯ Bild analysieren
            result = self._process_frame(frame, frame_idx)
            frame_results.append(result)
            
            # ğŸ’¾ Annotiertes Bild speichern (falls gewÃ¼nscht)
            if save_frames and output_dir and result.num_persons > 0:
                annotated_frame = draw_skeleton_filtered(
                    frame.copy(),
                    result.keypoints,
                    result.scores,
                    kpt_thr=self.kpt_threshold
                )
                frame_filename = output_dir / f"frame_{frame_idx:05d}.jpg"
                cv2.imwrite(str(frame_filename), annotated_frame)
            
            # ğŸ“Š Fortschritt anzeigen (alle 30 Bilder)
            if frame_idx % 30 == 0:
                print(f"ğŸ“Š Verarbeitet: {frame_idx}/{total_frames} Bilder")
        
        cap.release()  # ğŸ¬ Video schlieÃŸen
        
        processing_time = time.time() - start_time
        print(f"âœ… Fertig in {processing_time:.2f} Sekunden")
        
        return VideoResult(
            frame_results=frame_results,
            total_frames=len(frame_results),
            fps=fps,
            processing_time=processing_time
        )
    
    def export_to_json(
        self,
        result: Union[PoseResult, VideoResult],
        output_path: Union[str, Path],
        include_scores: bool = True  # ğŸ¯ Genauigkeiten mit speichern?
    ) -> None:
        """
        ğŸ“„ EXPORTIERT ERGEBNISSE ALS JSON-DATEI
        
        JSON ist wie ein digitales Notizbuch:
        - Menschen-lesbar
        - Computer-lesbar
        - Universell kompatibel
        
        ğŸ”§ Parameter:
            result: PoseResult oder VideoResult
            output_path: Wo JSON speichern?
            include_scores: Genauigkeiten mit exportieren?
        """
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ğŸ–¼ï¸ Einzelbild-Result
        if isinstance(result, PoseResult):
            data = {
                "frame_idx": int(result.frame_idx),
                "num_persons": int(result.num_persons),
                "keypoints": result.keypoints.tolist(),  # ğŸ”„ numpy â†’ Liste
                "bboxes": result.bboxes.tolist()
            }
            if include_scores:
                data["scores"] = result.scores.tolist()
        
        # ğŸï¸ Video-Result
        elif isinstance(result, VideoResult):
            data = {
                "total_frames": result.total_frames,
                "fps": result.fps,
                "processing_time": result.processing_time,
                "frames": []
            }
            
            # ğŸ”„ FÃ¼r jedes Bild im Video...
            for frame_result in result.frame_results:
                frame_data = {
                    "frame_idx": int(frame_result.frame_idx),
                    "num_persons": int(frame_result.num_persons),
                    "keypoints": frame_result.keypoints.tolist(),
                    "bboxes": frame_result.bboxes.tolist()
                }
                if include_scores:
                    frame_data["scores"] = frame_result.scores.tolist()
                
                data["frames"].append(frame_data)
        
        else:
            raise ValueError("âŒ Result muss PoseResult oder VideoResult sein")
        
        # ğŸ’¾ In Datei speichern
        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2)  # ğŸ“ SchÃ¶n formatiert (EinrÃ¼ckung: 2)
        
        print(f"ğŸ’¾ JSON exportiert: {output_path}")
    
    def get_summary(self, result: Union[PoseResult, VideoResult]) -> str:
        """
        ğŸ“‹ ERSTELLT EINE ZUSAMMENFASSUNG
        
        ğŸ”§ Parameter:
            result: PoseResult oder VideoResult
            
        ğŸ“¤ RÃ¼ckgabe:
            Formatierte Zusammenfassung als Text
        """
        if isinstance(result, PoseResult):
            # ğŸ–¼ï¸ Einzelbild-Zusammenfassung
            summary = f"=== Pose Estimation Summary ===\n"
            summary += f"Bild: {result.frame_idx}\n"
            summary += f"Gefundene Personen: {result.num_persons}\n"
            
            if result.num_persons > 0:
                for i in range(result.num_persons):
                    # ğŸ“Š Wie viele sichere Punkte?
                    valid_kpts = np.sum(result.scores[i] > self.kpt_threshold)
                    # ğŸ¯ Durchschnittliche Genauigkeit
                    avg_confidence = np.mean(result.scores[i][result.scores[i] > self.kpt_threshold])
                    summary += f"Person {i+1}: {valid_kpts}/133 Punkte, Genauigkeit: {avg_confidence:.1%}\n"
        
        elif isinstance(result, VideoResult):
            # ğŸï¸ Video-Zusammenfassung
            total_persons = sum(fr.num_persons for fr in result.frame_results)
            frames_with_detection = sum(1 for fr in result.frame_results if fr.num_persons > 0)
            
            summary = f"=== Video Processing Summary ===\n"
            summary += f"Gesamte Bilder: {result.total_frames}\n"
            summary += f"Bilder pro Sekunde: {result.fps:.2f}\n"
            summary += f"Verarbeitungszeit: {result.processing_time:.2f}s\n"
            summary += f"Bilder mit Personen: {frames_with_detection}/{result.total_frames}\n"
            summary += f"Gesamt Personen-Erkennungen: {total_persons}\n"
            
            if total_persons > 0:
                avg_persons_per_frame = total_persons / result.total_frames
                summary += f"Durchschnitt pro Bild: {avg_persons_per_frame:.2f} Personen\n"
        
        else:
            summary = "âŒ UngÃ¼ltiger Result-Typ"
        
        return summary

# ===============================================
# âš¡ BEQUEMLICHKEITSFUNKTIONEN (Schnellstart)
# ===============================================
# Diese Funktionen sind fÃ¼r "Ich will jetzt sofort loslegen!"

def estimate_pose_image(
    image_path: Union[str, Path],
    output_path: Optional[Union[str, Path]] = None,
    mode: str = 'performance',  # ğŸ¥‡ Beste Genauigkeit
    device: str = 'cpu'         # ğŸ’» Auf CPU laufen lassen
) -> PoseResult:
    """
    âš¡ SCHNELLE FUNKTION FÃœR EINZELBILD-ANALYSE
    
    BEISPIEL:
        result = estimate_pose_image("urlaub.jpg", "urlaub_pose.jpg")
    
    ğŸ”§ Parameter:
        image_path: Pfad zum Bild
        output_path: Wo annotiertes Bild speichern? (optional)
        mode: KI-Modus ('performance', 'balanced', 'lightweight')
        device: Hardware ('cpu', 'cuda', 'mps')
        
    ğŸ“¤ RÃ¼ckgabe:
        PoseResult
    """
    # ğŸ¤– Estimator erstellen
    estimator = PoseEstimator2D(mode=mode, device=device)
    
    # ğŸ¯ Bild analysieren (mit oder ohne Annotation)
    if output_path:
        return estimator.process_image_with_annotation(image_path, output_path)
    else:
        return estimator.process_image(image_path)

def estimate_pose_video(
    video_path: Union[str, Path],
    output_dir: Optional[Union[str, Path]] = None,
    mode: str = 'performance',
    device: str = 'cpu',
    max_frames: Optional[int] = None
) -> VideoResult:
    """
    âš¡ SCHNELLE FUNKTION FÃœR VIDEO-ANALYSE
    
    BEISPIEL:
        result = estimate_pose_video("tanzen.mp4", "ergebnisse/")
    
    ğŸ”§ Parameter:
        video_path: Pfad zum Video
        output_dir: Wo Ergebnisse speichern? (optional)
        mode: KI-Modus
        device: Hardware
        max_frames: Maximale Anzahl Bilder
        
    ğŸ“¤ RÃ¼ckgabe:
        VideoResult
    """
    estimator = PoseEstimator2D(mode=mode, device=device)
    return estimator.process_video(
        video_path,
        output_dir=output_dir,
        save_frames=bool(output_dir),  # ğŸ’¾ Nur speichern wenn output_dir gegeben
        max_frames=max_frames
    )

# ===============================================
# ğŸš€ START: WENN DAS PROGRAMM DIREKT GESTARTET WIRD
# ===============================================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ¤– RTMLib Pose Estimator 2D - Test Script")
    print("=" * 60)
    print("ğŸ“ Testet die Posenerkennung mit einem Beispielbild")
    print("")
    
    # ğŸ” Testbild suchen
    test_image = Path("../data/test_pose.png")
    
    if test_image.exists():
        print(f"âœ… Testbild gefunden: {test_image}")
        print("")
        
        # ğŸ¤– Estimator erstellen (ausgewogener Modus, auf CPU)
        print("1ï¸âƒ£  Erstelle Pose-Estimator...")
        estimator = PoseEstimator2D(mode='balanced', device='cpu')
        
        # ğŸ¯ Bild analysieren
        print("2ï¸âƒ£  Analysiere Bild...")
        result = estimator.process_image(test_image)
        
        # ğŸ“Š Zusammenfassung anzeigen
        print("3ï¸âƒ£  Zeige Zusammenfassung:")
        print(estimator.get_summary(result))
        
        # ğŸ–ï¸ Annotiertes Ergebnis speichern
        print("4ï¸âƒ£  Speichere annotiertes Bild...")
        output_path = Path("../output/pose-estimation/test_result.png")
        estimator.process_image_with_annotation(test_image, output_path)
        
        # ğŸ“„ In JSON exportieren
        print("5ï¸âƒ£  Exportiere als JSON...")
        json_path = Path("../output/pose-estimation/test_result.json")
        estimator.export_to_json(result, json_path)
        
        print("")
        print("=" * 40)
        print("ğŸ‰ Alles fertig! Ergebnisse im Ordner 'output/'")
        print("")
        print("ğŸ“ Du findest:")
        print("   - test_result.png (Bild mit eingezeichneten Personen)")
        print("   - test_result.json (Daten aller KÃ¶rperpunkte)")
        
    else:
        print(f"âš ï¸  Kein Testbild gefunden: {test_image}")
        print("")
        print("â„¹ï¸  So kannst du es trotzdem nutzen:")
        print("")
        print("ğŸ–¼ï¸  FÃœR BILDER:")
        print("    result = estimate_pose_image('mein_bild.jpg')")
        print("    result = estimate_pose_image('mein_bild.jpg', 'ergebnis.jpg')")
        print("")
        print("ğŸï¸  FÃœR VIDEOS:")
        print("    result = estimate_pose_video('mein_video.mp4', 'ergebnisse/')")
        print("")
        print("ğŸ’¡ TIPPS:")
        print("    - Verwende mode='lightweight' fÃ¼r schnellere Analyse")
        print("    - Verwende device='cuda' falls du NVIDIA GPU hast")
        print("    - max_frames=100 fÃ¼r schnellen Test mit Videos")