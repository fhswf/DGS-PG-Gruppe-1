"""
ğŸ”„ Pose3DConverter: 2D â†’ 3D KÃ¶rperpositionen konvertieren

EINFACHE ERKLÃ„RUNG:
Dieses Programm nimmt 2D-Bilder mit erkannten KÃ¶rperpunkten 
und macht daraus 3D-Modelle! Stell es dir vor wie:

ğŸ–¼ï¸ 2D-Foto â†’ ğŸª„ Magie â†’ ğŸ¯ 3D-Figur

Es funktioniert so:
1. Nimmt Punkte von einem 2D-Bild (X, Y Koordinaten)
2. SchÃ¤tzt die Tiefe (Z-Koordinate)
3. Erstellt daraus eine 3D-Figur, die man von allen Seiten betrachten kann

Besonderheit: BehÃ¤lt alle wichtigen Punkte bei - genau wie im 2D-Wrapper!
"""

# ===============================================
# ğŸ“¦ IMPORTIEREN DER BENÃ–TIGTEN BIBLIOTHEKEN
# ===============================================
import numpy as np  # ğŸ”¢ FÃ¼r Mathe und 3D-Berechnungen
from pathlib import Path  # ğŸ“ FÃ¼r Dateipfade
from typing import Union, List, Tuple, Optional, Dict  # ğŸ“ FÃ¼r bessere Code-Lesbarkeit
from dataclasses import dataclass  # ğŸ—ï¸ FÃ¼r strukturierte Daten-Container
import json  # ğŸ“„ Zum Speichern im JSON-Format
import warnings  # âš ï¸ FÃ¼r Warnmeldungen

try:
    # ğŸ¤– Versuche MMPose zu laden (fortgeschrittene 3D-PosenschÃ¤tzung)
    from mmpose.apis import MMPoseInferencer
    MMPOSE_AVAILABLE = True
except ImportError:
    # â„¹ï¸ Falls nicht verfÃ¼gbar: trotzdem weitermachen (geometrische Methode geht immer)
    MMPOSE_AVAILABLE = False

# ===============================================
# âš™ï¸ KONFIGURATION: WELCHE KÃ–RPERTEILE WEGLASSEN?
# ===============================================
# StandardmÃ¤ÃŸig ignorierte KÃ¶rperpunkte: Beine, FÃ¼ÃŸe, Zehen (Punkte 13-22)
DEFAULT_IGNORE_KEYPOINTS = list(range(13, 23))

# ===============================================
# ğŸ“¦ DATENKLASSE: 3D-ERGEBNISSE SPEICHERN
# ===============================================
@dataclass
class Pose3DResult:
    """
    ğŸ·ï¸ EIN "DATEN-BEHÃ„LTER" FÃœR 3D-ERGEBNISSE
    
    Speichert alle Informationen zu einer 3D-KÃ¶rperposition.
    
    ğŸ“‹ INHALT:
        frame_idx:      Bild-Nummer
        keypoints_3d:   3D-KÃ¶rperpunkte [Personen, 133 Punkte, X/Y/Z]
        keypoints_2d:   Original 2D-Punkte [Personen, 133 Punkte, X/Y]
        scores_3d:      Genauigkeiten in 3D [Personen, 133 Punkte]
        bboxes_3d:      3D-Begrenzungsrahmen [Personen, 7 Werte]
        num_persons:    Anzahl der Personen
        method:         Welche Methode wurde verwendet?
        confidence:     Durchschnittliche Genauigkeit
    """
    frame_idx: int
    keypoints_3d: np.ndarray
    keypoints_2d: np.ndarray
    scores_3d: np.ndarray
    bboxes_3d: np.ndarray
    num_persons: int
    method: str
    confidence: float

# ===============================================
# ğŸ”„ HAUPTKLASSE: DER 3D-KONVERTER
# ===============================================
class Pose3DConverter:
    """
    ğŸª„ DIE HAUPTKLASSE FÃœR 2Dâ†’3D KONVERTIERUNG
    
    EINFACH GESAGT:
    Nimmt flache 2D-Bilder und macht sie "tief" - wie aus einem Foto 
    eine kleine 3D-Figur fÃ¼r ein Computerspiel zu machen.
    
    So funktioniert die "Magie":
    1. ğŸ“ Kopiert alle 2D-Punkte (X, Y)
    2. ğŸ” SchÃ¤tzt fÃ¼r jeden Punkt die Tiefe (Z)
    3. ğŸ¯ BehÃ¤lt alle wichtigen Punkte bei (HÃ¤nde, Gesicht)
    4. ğŸš« Entfernt Beine (wenn gewÃ¼nscht)
    """
    
    def __init__(
        self,
        lifting_method: str = 'geometric',  # ğŸ”§ Methode: 'geometric' (einfach & zuverlÃ¤ssig)
        mmpose_model: str = 'human3d',      # ğŸ¤– Fortgeschrittenes KI-Modell (optional)
        mmpose_weights: Optional[str] = None,  # âš–ï¸ KI-Gewichte (falls KI verwendet)
        device: str = 'cpu',                # ğŸ’» Hardware (cpu, cuda fÃ¼r NVIDIA)
        ignore_keypoints: Optional[List[int]] = None,  # ğŸš« Zu ignorierende Punkte
        image_width: int = 1920,            # ğŸ“ Standard-Bildbreite
        image_height: int = 1080            # ğŸ“ Standard-BildhÃ¶he
    ):
        """
        ğŸ—ï¸ KONSTRUKTOR: INITIALISIERT DEN 3D-KONVERTER
        
        Hier wird festgelegt, wie die Konvertierung funktionieren soll.
        """
        self.lifting_method = lifting_method
        self.device = device
        self.ignore_keypoints = ignore_keypoints if ignore_keypoints is not None else DEFAULT_IGNORE_KEYPOINTS
        self.image_width = image_width
        self.image_height = image_height
        self.lifting_method = 'geometric'  # ğŸ¯ IMMER geometrische Methode (konsistent)
        
        print(f"âœ… Pose3DConverter bereit!")
        print(f"   Methode: {self.lifting_method}")
        print(f"   Ignoriere Punkte: {self.ignore_keypoints}")
        print(f"   BildgrÃ¶ÃŸe: {image_width}x{image_height}")
    
    def _copy_keypoints_from_2d(self, keypoints_2d: np.ndarray, scores_2d: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        ğŸ“‹ KOPIERT 2D-PUNKTE FÃœR 3D - OHNE Ã„NDERUNGEN!
        
        WICHTIGSTE REGEL: "Mach genau das gleiche wie im 2D-Wrapper!"
        
        Warum?
        - Im 2D-Wrapper wurden schon Punkte ersetzt (Nase, Handgelenke)
        - Diese Ã„nderungen mÃ¼ssen in 3D beibehalten werden
        - Sonst passen 2D und 3D nicht zusammen!
        
        ğŸ”§ Parameter:
            keypoints_2d: 2D-Punkte vom Wrapper
            scores_2d:    2D-Genauigkeiten vom Wrapper
            
        ğŸ“¤ RÃ¼ckgabe:
            GLEICHE keypoints_2d und scores_2d (als Kopien)
        """
        return keypoints_2d.copy(), scores_2d.copy()  # ğŸ“‹ Einfach kopieren!
    
    def convert_2d_to_3d(
        self,
        keypoints_2d: np.ndarray,  # ğŸ“ Eingabe: 2D-Punkte
        scores_2d: np.ndarray,     # ğŸ¯ Eingabe: 2D-Genauigkeiten
        image_size: Tuple[int, int] = None  # ğŸ“ Optional: BildgrÃ¶ÃŸe Ã¼berschreiben
    ) -> Pose3DResult:
        """
        ğŸª„ HAUPT-FUNKTION: WANDELT 2D IN 3D UM
        
        Ablauf der "Magie":
        1. ğŸ“‹ Kopiere 2D-Punkte (KEINE Ã„nderungen!)
        2. ğŸ” FÃ¼ge Tiefe (Z-Koordinate) hinzu
        3. ğŸš« Filtere unerwÃ¼nschte Punkte (Beine)
        4. ğŸ“¦ Berechne 3D-Begrenzungsrahmen
        5. ğŸ“Š Berechne Gesamt-Genauigkeit
        
        ğŸ”§ Parameter:
            keypoints_2d: 2D-KÃ¶rperpunkte [Personen, 133, 2]
            scores_2d:    2D-Genauigkeiten [Personen, 133]
            image_size:   (Breite, HÃ¶he) des Originalbildes
            
        ğŸ“¤ RÃ¼ckgabe:
            Pose3DResult mit allen 3D-Daten
        """
        # ğŸš« PrÃ¼fen: Sind Ã¼berhaupt Personen vorhanden?
        if len(keypoints_2d) == 0:
            return self._empty_result()  # ğŸ“­ Leeres Ergebnis zurÃ¼ckgeben
        
        # ğŸ“ BildgrÃ¶ÃŸe festlegen (Standard oder angegeben)
        w, h = image_size if image_size else (self.image_width, self.image_height)
        
        # ===============================================
        # ğŸ“‹ SCHRITT 1: 2D-PUNKTE KOPIEREN (OHNE Ã„NDERUNGEN!)
        # ===============================================
        kpts_2d, scores = self._copy_keypoints_from_2d(keypoints_2d, scores_2d)
        
        # ===============================================
        # ğŸ”„ SCHRITT 2: 2D â†’ 3D KONVERTIEREN ("Magie!")
        # ===============================================
        keypoints_3d_list = []  # ğŸ“‹ FÃ¼r 3D-Punkte jeder Person
        scores_3d_list = []     # ğŸ“‹ FÃ¼r 3D-Genauigkeiten
        
        # ğŸ‘¥ FÃ¼r jede Person...
        for person_idx in range(len(kpts_2d)):
            kpts = kpts_2d[person_idx]  # ğŸ“ 2D-Punkte dieser Person
            scr = scores[person_idx]    # ğŸ¯ Genauigkeiten dieser Person
            
            # ğŸª„ Geometrische Konvertierung: 2D â†’ 3D
            kpts_3d = self._geometric_lift_2d_to_3d(kpts, scr, (h, w))
            
            keypoints_3d_list.append(kpts_3d)  # ğŸ’¾ 3D-Punkte speichern
            scores_3d_list.append(scr)         # ğŸ’¾ Genauigkeiten behalten
        
        # ğŸ”¢ In numpy Arrays umwandeln
        keypoints_3d = np.array(keypoints_3d_list)
        scores_3d = np.array(scores_3d_list)
        
        # ===============================================
        # ğŸš« SCHRITT 3: PUNKTE FILTERN (Beine entfernen)
        # ===============================================
        keypoints_3d, scores_3d = self._filter_keypoints(keypoints_3d, scores_3d)
        
        # ===============================================
        # ğŸ“¦ SCHRITT 4: 3D-BEGRENZUNGSRAHMEN BERECHNEN
        # ===============================================
        bboxes_3d = self._calculate_3d_bboxes(keypoints_3d, scores_3d)
        
        # ===============================================
        # ğŸ“Š SCHRITT 5: GESAMT-GENAUIGKEIT BERECHNEN
        # ===============================================
        if np.any(scores_3d > 0):  # ğŸ¯ Falls gÃ¼ltige Punkte existieren
            confidence = float(np.mean(scores_3d[scores_3d > 0]))
        else:
            confidence = 0.0  # ğŸš« Keine gÃ¼ltigen Punkte
        
        # ===============================================
        # ğŸ” SCHRITT 6: DEBUG-AUSGABE (FÃ¼r Entwickler)
        # ===============================================
        if len(keypoints_3d) > 0:
            print(f"\nğŸ” 3D-Konvertierung - Wichtige Punkte prÃ¼fen:")
            print(f"   Punkt 9 (L-Handgelenk): Pos={keypoints_3d[0, 9]}, Score={scores_3d[0, 9]:.3f}")
            print(f"   Punkt 91 (L-Handwurzel): Pos={keypoints_3d[0, 91]}, Score={scores_3d[0, 91]:.3f}")
            print(f"   âœ… Sind sie gleich? {np.array_equal(keypoints_3d[0, 9], keypoints_3d[0, 91])}")
            
            print(f"\n   Punkt 10 (R-Handgelenk): Pos={keypoints_3d[0, 10]}, Score={scores_3d[0, 10]:.3f}")
            print(f"   Punkt 112 (R-Handwurzel): Pos={keypoints_3d[0, 112]}, Score={scores_3d[0, 112]:.3f}")
            print(f"   âœ… Sind sie gleich? {np.array_equal(keypoints_3d[0, 10], keypoints_3d[0, 112])}")
        
        # ===============================================
        # ğŸ“¤ SCHRITT 7: ERGEBNIS ZURÃœCKGEBEN
        # ===============================================
        return Pose3DResult(
            frame_idx=0,
            keypoints_3d=keypoints_3d,      # ğŸ¯ 3D-Punkte
            keypoints_2d=kpts_2d,           # ğŸ“ Original 2D-Punkte
            scores_3d=scores_3d,            # ğŸ¯ 3D-Genauigkeiten
            bboxes_3d=bboxes_3d,            # ğŸ“¦ 3D-Begrenzungsrahmen
            num_persons=len(keypoints_2d),  # ğŸ‘¥ Personen-Anzahl
            method=self.lifting_method,     # ğŸ”§ Verwendete Methode
            confidence=confidence           # ğŸ“Š Gesamt-Genauigkeit
        )
    
    def _geometric_lift_2d_to_3d(
        self, 
        keypoints_2d: np.ndarray, 
        scores: np.ndarray, 
        image_shape: Tuple[int, int]
    ) -> np.ndarray:
        """
        ğŸª„ GEOMETRISCHE 2Dâ†’3D KONVERTIERUNG
        
        EINFACH GESAGT:
        "Wie tief ist jeder KÃ¶rperteil?" - SchÃ¤tzung basierend auf Position.
        
        ğŸ® SO WIRD TIEFE GESCHÃ„TZT:
        - Nase, Augen, Ohren:      Etwas vorne (Z = 0.1)
        - Schultern, Ellbogen:     In der Mitte (Z = 0.0)
        - HÃ¤nde, Finger:           Weit vorne (Z = 0.2-0.25)
        - Gesicht:                 Vorne (Z = 0.1)
        - Beine:                   Weiter hinten (aber werden ignoriert)
        
        ğŸ”§ Parameter:
            keypoints_2d: Einzelne Person, 133 Punkte, [X, Y]
            scores:       Genauigkeiten der Punkte
            image_shape:  (HÃ¶he, Breite) des Bildes
            
        ğŸ“¤ RÃ¼ckgabe:
            3D-Punkte [133 Punkte, X/Y/Z]
        """
        h, w = image_shape
        keypoints_3d = np.zeros((133, 3))  # ğŸ†• Leeres 3D-Array
        
        # ğŸ“‹ 1. X und Y KOORDINATEN KOPIEREN (aus 2D)
        keypoints_3d[:, :2] = keypoints_2d
        
        # ğŸ” 2. TIEFE (Z) FÃœR JEDEN PUNKT SCHÃ„TZEN
        for i in range(133):
            if scores[i] > 0.3:  # ğŸ¯ Nur bei ausreichender Genauigkeit
                if i == 0:  # ğŸ‘ƒ Nase (vorne)
                    keypoints_3d[i, 2] = 0.1
                elif 1 <= i <= 4:  # ğŸ‘€ Augen, ğŸ‘‚ Ohren (vorne)
                    keypoints_3d[i, 2] = 0.1
                elif 5 <= i <= 12:  # ğŸ’ª Schultern, Ellbogen, ğŸ‘ HÃ¼ften (Mitte)
                    keypoints_3d[i, 2] = 0.0
                elif i in [9, 10]:  # âœ‹ Handgelenke (WICHTIG! vorne)
                    keypoints_3d[i, 2] = 0.2
                elif 91 <= i <= 111:  # âœ‹ Linke Hand (Finger, sehr vorne)
                    keypoints_3d[i, 2] = 0.25
                elif 112 <= i <= 132:  # âœ‹ Rechte Hand (Finger, sehr vorne)
                    keypoints_3d[i, 2] = 0.25
                elif 23 <= i <= 90:  # ğŸ˜€ Gesicht (vorne)
                    keypoints_3d[i, 2] = 0.1
                else:  # ğŸ¦µ Andere Punkte (Beine, werden ignoriert)
                    keypoints_3d[i, 2] = 0.0
        
        return keypoints_3d
    
    def _filter_keypoints(
        self, 
        keypoints: np.ndarray, 
        scores: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        ğŸš« FILTERT IGNORIERTE KÃ–RPERPUNKTE (Z.B. BEINE)
        
        Setzt ignorierte Punkte auf (0,0,0) und Genauigkeit auf 0.
        Das macht sie praktisch "unsichtbar" in der 3D-Visualisierung.
        """
        kpts = keypoints.copy()  # ğŸ“‹ Kopie (Original bleibt unverÃ¤ndert)
        scrs = scores.copy()     # ğŸ“‹ Kopie
        
        # ğŸ”„ FÃ¼r jeden zu ignorierenden Punkt...
        for idx in self.ignore_keypoints:
            if idx < kpts.shape[1]:  # âœ… PrÃ¼fen ob Punkt existiert
                kpts[:, idx, :] = 0  # ğŸ¯ Position auf (0,0,0)
                scrs[:, idx] = 0     # ğŸ¯ Genauigkeit auf 0
        
        return kpts, scrs
    
    def _calculate_3d_bboxes(
        self, 
        keypoints_3d: np.ndarray, 
        scores_3d: np.ndarray
    ) -> np.ndarray:
        """
        ğŸ“¦ BERECHNET 3D-BEGRENZUNGSRAHMEN ("BOUNDING BOXES")
        
        Ein 3D-Rahmen ist wie eine imaginÃ¤re Schachtel um die Person.
        EnthÃ¤lt: [Mittelpunkt-X, Mittelpunkt-Y, Mittelpunkt-Z, 
                  Breite, HÃ¶he, Tiefe, Genauigkeit]
        """
        bboxes = []
        
        # ğŸ‘¥ FÃ¼r jede Person...
        for i in range(len(keypoints_3d)):
            # ğŸ¯ Nur Punkte mit guter Genauigkeit berÃ¼cksichtigen
            valid_mask = scores_3d[i] > 0.3
            valid_kpts = keypoints_3d[i][valid_mask]
            
            if len(valid_kpts) > 0:
                # ğŸ“ Minimum und Maximum in allen 3 Dimensionen
                min_coords = np.min(valid_kpts, axis=0)  # ğŸ”½ Kleinste X,Y,Z
                max_coords = np.max(valid_kpts, axis=0)  # ğŸ”¼ GrÃ¶ÃŸte X,Y,Z
                
                # ğŸ¯ Mittelpunkt berechnen
                center = (min_coords + max_coords) / 2
                
                # ğŸ“ Abmessungen (Breite, HÃ¶he, Tiefe)
                dimensions = max_coords - min_coords
                
                # ğŸ¯ Durchschnittliche Genauigkeit
                confidence = np.mean(scores_3d[i][valid_mask])
                
                # ğŸ“¦ Alles zusammenfÃ¼gen [X,Y,Z, Breite, HÃ¶he, Tiefe, Genauigkeit]
                bboxes.append(np.concatenate([center, dimensions, [confidence]]))
            else:
                # ğŸš« Keine gÃ¼ltigen Punkte: leeren Rahmen
                bboxes.append(np.zeros(7))
        
        return np.array(bboxes)  # ğŸ”„ In numpy Array
    
    def _empty_result(self) -> Pose3DResult:
        """
        ğŸ“­ GIBT EIN LEERES ERGEBNIS ZURÃœCK
        
        Wird verwendet, wenn keine Personen gefunden wurden.
        """
        return Pose3DResult(
            frame_idx=0,
            keypoints_3d=np.empty((0, 133, 3)),  # ğŸ“­ 0 Personen
            keypoints_2d=np.empty((0, 133, 2)),
            scores_3d=np.empty((0, 133)),
            bboxes_3d=np.empty((0, 7)),
            num_persons=0,
            method=self.lifting_method,
            confidence=0.0
        )
    
    def convert_2d_json_to_3d(
        self,
        input_json_path: Union[str, Path],  # ğŸ“ Eingabe: 2D-JSON
        output_json_path: Union[str, Path],  # ğŸ“ Ausgabe: 3D-JSON
        image_size: Tuple[int, int] = None   # ğŸ“ Optional: BildgrÃ¶ÃŸe
    ) -> List[Dict]:
        """
        ğŸ“ KONVERTIERT EINE GANZE 2D-JSON-DATEI ZU 3D
        
        Liest eine JSON-Datei mit 2D-Posen (von PoseEstimator2D)
        und erstellt eine neue JSON-Datei mit 3D-Posen.
        
        ğŸ”§ Parameter:
            input_json_path:  Pfad zur 2D-JSON-Datei
            output_json_path: Wo 3D-JSON gespeichert werden soll
            image_size:       BildgrÃ¶ÃŸe fÃ¼r alle Frames
            
        ğŸ“¤ RÃ¼ckgabe:
            Liste mit allen 3D-Ergebnissen
        """
        input_path = Path(input_json_path)
        if not input_path.exists():
            raise FileNotFoundError(f"âŒ 2D JSON nicht gefunden: {input_path}")
        
        # ğŸ“– 2D-Daten laden
        with open(input_path, 'r') as f:
            data_2d = json.load(f)
        
        results_3d = []  # ğŸ“‹ FÃ¼r alle 3D-Ergebnisse
        
        print(f"ğŸ”„ Konvertiere {len(data_2d)} Bilder von 2D zu 3D...")
        
        # ğŸ”„ FÃ¼r jedes Bild in der 2D-Datei...
        for frame_data in data_2d:
            frame_idx = frame_data['frame']  # ğŸï¸ Bild-Nummer
            
            # ğŸ¯ Linke Ansicht konvertieren (von Stereo-Kamera)
            left_3d = self._convert_single_view(frame_data['left'], image_size, frame_idx)
            
            # ğŸ¯ Rechte Ansicht konvertieren
            right_3d = self._convert_single_view(frame_data['right'], image_size, frame_idx)
            
            # ğŸ¯ Kombinierte Ansicht (hier einfach linke nehmen)
            frame_result = {
                "frame": frame_idx,
                "left_3d": left_3d,      # ğŸ‘ˆ Linke Kamera-Ansicht
                "right_3d": right_3d,    # ğŸ‘‰ Rechte Kamera-Ansicht
                "combined_3d": left_3d   # ğŸ¯ Beste kombinierte Ansicht
            }
            results_3d.append(frame_result)
            
            # ğŸ“Š Fortschritt anzeigen (alle 10 Bilder)
            if frame_idx % 10 == 0:
                print(f"  ğŸ“Š Bild {frame_idx}/{len(data_2d)} konvertiert")
        
        # ğŸ’¾ 3D-Daten speichern
        with open(output_json_path, 'w') as f:
            json.dump(results_3d, f, indent=2)  # ğŸ“ SchÃ¶n formatiert
        
        print(f"âœ… 3D Posen gespeichert: {output_json_path}")
        return results_3d
    
    def _convert_single_view(
        self, 
        view_data: Dict, 
        image_size: Tuple[int, int], 
        frame_idx: int
    ) -> Dict:
        """
        ğŸ”„ KONVERTIERT EINE EINZELNE KAMERA-ANSICHT
        
        Wird fÃ¼r linke und rechte Kamera-Ansicht aufgerufen.
        """
        # ğŸ“ 2D-Punkte extrahieren
        keypoints_2d = np.array(view_data['keypoints'])
        scores_2d = np.array(view_data['scores'])
        
        # ğŸª„ 2D â†’ 3D konvertieren
        pose_3d = self.convert_2d_to_3d(keypoints_2d, scores_2d, image_size)
        
        # ğŸ“¦ Ergebnis als Dictionary zurÃ¼ckgeben
        return {
            "keypoints_3d": pose_3d.keypoints_3d.tolist(),  # ğŸ”„ numpy â†’ Liste
            "scores_3d": pose_3d.scores_3d.tolist(),
            "bboxes_3d": pose_3d.bboxes_3d.tolist(),
            "num_persons": pose_3d.num_persons,
            "method": pose_3d.method,
            "confidence": pose_3d.confidence
        }

# ===============================================
# âš¡ BEQUEMLICHKEITSFUNKTION (FÃ¼r Import)
# ===============================================
# Diese Funktion wird vom Test-Skript importiert!

def convert_2d_poses_to_3d(
    input_json_path: Union[str, Path],  # ğŸ“ Eingabe: 2D-Posen
    output_json_path: Union[str, Path],  # ğŸ“ Ausgabe: 3D-Posen
    lifting_method: str = 'geometric',   # ğŸ”§ Methode (immer geometric)
    mmpose_model: str = 'human3d',       # ğŸ¤– KI-Modell (optional)
    device: str = 'cpu'                  # ğŸ’» Hardware
) -> List[Dict]:
    """
    âš¡ SCHNELLE FUNKTION FÃœR 2Dâ†’3D KONVERTIERUNG
    
    Diese Funktion wird vom Test-Skript aufgerufen.
    Einfachste Nutzung:
        convert_2d_poses_to_3d("2d_poses.json", "3d_poses.json")
    
    ğŸ”§ Parameter:
        input_json_path:  2D-JSON-Datei von PoseEstimator2D
        output_json_path: Wo 3D-JSON gespeichert werden soll
        lifting_method:   Konvertierungs-Methode
        mmpose_model:     KI-Modell fÃ¼r fortgeschrittene Methoden
        device:           Hardware (cpu, cuda)
        
    ğŸ“¤ RÃ¼ckgabe:
        Liste mit allen 3D-Ergebnissen
    """
    # ğŸ¤– Konverter erstellen
    converter = Pose3DConverter(
        lifting_method=lifting_method,
        mmpose_model=mmpose_model,
        device=device
    )
    # ğŸ”„ Konvertierung durchfÃ¼hren
    return converter.convert_2d_json_to_3d(input_json_path, output_json_path)

# ===============================================
# ğŸš€ START: WENN DAS PROGRAMM DIREKT GESTARTET WIRD
# ===============================================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ”„ Pose3DConverter - Test Script")
    print("=" * 60)
    print("ğŸ“ Testet die 2Dâ†’3D Konvertierung")
    print("")
    
    # ğŸ” Test-Dateien
    test_input = "poses_2d_filtered.json"  # ğŸ“ Erwartet: 2D-Posen
    test_output = "poses_3d_test.json"     # ğŸ“ Wird erstellt: 3D-Posen
    
    if Path(test_input).exists():
        print(f"âœ… Testdatei gefunden: {test_input}")
        print("ğŸ”„ Starte Konvertierung...")
        
        # ğŸª„ Konvertierung durchfÃ¼hren
        results = convert_2d_poses_to_3d(test_input, test_output)
        
        print(f"âœ… Erfolgreich konvertiert!")
        print(f"   ğŸ“Š {len(results)} Bilder verarbeitet")
        print(f"   ğŸ“ Ergebnis: {test_output}")
        
    else:
        print(f"âš ï¸  Testdatei {test_input} nicht gefunden")
        print("")
        print("â„¹ï¸  So nutzt du es:")
        print("   1. Erstelle zuerst 2D-Posen mit PoseEstimator2D")
        print("   2. Speichere sie als JSON (z.B. 'poses_2d.json')")
        print("   3. Konvertiere zu 3D:")
        print("      convert_2d_poses_to_3d('poses_2d.json', 'poses_3d.json')")
        print("")
        print("ğŸ’¡ TIPP: Die 3D-Datei kann mit dem 3D Visualizer angezeigt werden!")